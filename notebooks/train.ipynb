{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framework imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from noronha.tools.publish import Publisher\n",
    "from noronha.tools.shortcuts import data_path, tmp_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "train_dataset = load_dataset('csv', data_files='datasets/atis_intents_train.csv')\n",
    "eval_dataset = load_dataset('csv', data_files='adatasets/tis_intents_test.csv')\n",
    "\n",
    "\n",
    "dataset = load_dataset('csv', data_files={'train': 'atis_intents_train.csv', 'test': 'atis_intents_test.csv'}, encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [item[\"text\"] for item in dataset[\"train\"]]\n",
    "train_labels = [item[\"label\"] for item in dataset[\"train\"]]\n",
    "\n",
    "test_texts = [item[\"text\"] for item in dataset[\"test\"]]\n",
    "test_labels = [item[\"label\"] for item in dataset[\"test\"]]\n",
    "\n",
    "label_names = ['atis_flight',          \n",
    "'atis_airfare',\n",
    "'atis_ground_service',\n",
    "'atis_airline',\n",
    "'atis_abbreviation',\n",
    "'atis_aircraft',\n",
    "'atis_flight_time',\n",
    "'atis_quantity',\n",
    "'atis_airport',\n",
    "'atis_distance',\n",
    "'atis_city',\n",
    "'atis_ground_fare',\n",
    "'atis_capacity',\n",
    "'atis_flight_no',\n",
    "'atis_meal',\n",
    "'atis_restriction',\n",
    "'atis_cheapest']\n",
    "\n",
    "train_texts, dev_texts, train_labels, dev_labels = train_test_split(train_texts, \n",
    "                                                                    train_labels, \n",
    "                                                                    test_size=0.2, \n",
    "                                                                    shuffle=True, \n",
    "                                                                    random_state=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['label'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_num(word_list):\n",
    "    word_to_number_map = {}\n",
    "    number = 0\n",
    "    result = []\n",
    "    for word in word_list:\n",
    "        if word not in word_to_number_map:\n",
    "            word_to_number_map[word] = number\n",
    "            number += 1\n",
    "        result.append(word_to_number_map[word])\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model & the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"albert-base-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=len(label_names))\n",
    "\n",
    "train_texts_encoded = tokenizer(train_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "dev_texts_encoded = tokenizer(dev_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "test_texts_encoded = tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "train_labels = label_to_num(train_labels)\n",
    "dev_labels = label_to_num(dev_labels)\n",
    "test_labels = label_to_num(test_labels)\n",
    "\n",
    "train_dataset = ClassificationDataset(train_texts_encoded, train_labels)\n",
    "dev_dataset = ClassificationDataset(dev_texts_encoded, dev_labels)\n",
    "test_dataset = ClassificationDataset(test_texts_encoded, test_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_steps=int(len(train_dataset)/16),\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        save_steps=50,\n",
    "        save_total_limit=10,\n",
    "        load_best_model_at_end=True,\n",
    "        save_strategy=\"no\",\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "test_results = trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, tmp_path('model.pkl')) # model pushing\n",
    "\n",
    "Publisher()(\n",
    "    details=dict(\n",
    "        metrics=test_results\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b19694c2477b6961a9baaea990607e8a5bdfd660487bf673882fbc4439b85657"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
